{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from data_utils import Dictionary, Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "# Hyper-parameters\n",
    "\n",
    "embed_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "num_epochs = 5\n",
    "batch_size = 20\n",
    "seq_length = 30\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval\n",
    "\n",
    "Read the data from the files, which is basically assigning word IDs and making batches.\n",
    "\n",
    "I have used Penn Treebank from Mikolov's webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus()\n",
    "word_ids = corpus.get_data(\"data/train.txt\", batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "number_batches = word_ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Definition\n",
    "\n",
    "A simple multi-layer LSTM with dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout_prob = 0.5):\n",
    "        super(RNN, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout_prob) # create inline dropout function\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size) # create embedding matrix\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout = dropout_prob, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.drop(self.embeddings(x))  # dropout before LSTM pipeline\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        out = self.drop(out) \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0) * out.size(1), out.size(2))\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.load_state_dict(torch.load('model.ckpt')) # load previous model\n",
    "model.lstm.flatten_parameters() # unrolling parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step[0/1549], Loss: 9.1282, Perplexity: 9211.32\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # turn the model into training mode\n",
    "        \n",
    "        #initialize the states\n",
    "        states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "                  torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "\n",
    "        for i in range(0, word_ids.size(1) - seq_length, seq_length):\n",
    "            # batchifying the sequence\n",
    "            inputs = word_ids[:, i: i + seq_length].to(device)\n",
    "            targets = word_ids[:, (i + 1) : (i + 1) + seq_length].to(device)\n",
    "\n",
    "            # detaching the states every batch\n",
    "            # idea of truncated backprop\n",
    "            states = repackage_hidden(states)\n",
    "            outputs, states = model(inputs, states)\n",
    "            loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # avoiding gradient explosion\n",
    "            clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            # log errors after every 100 steps\n",
    "            step = (i + 1) // seq_length\n",
    "            if step % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                       .format(epoch+1, num_epochs, step, number_batches, loss.item(), np.exp(loss.item())))\n",
    "        \n",
    "        # after every epoch run the validation step\n",
    "        validate(model, \"valid\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data):\n",
    "    model.eval() # turn the model to evalutation mode\n",
    "    eval_batch_size = 1\n",
    "    states = (torch.zeros(num_layers, eval_batch_size, hidden_size).to(device),\n",
    "              torch.zeros(num_layers, eval_batch_size, hidden_size).to(device))\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    # get data item by item from validation set\n",
    "    if data == \"valid\":\n",
    "        test_ids = corpus.get_data(\"data/valid.txt\", eval_batch_size)\n",
    "    else:\n",
    "        test_ids = corpus.get_data(\"data/test.txt\", eval_batch_size)\n",
    "    \n",
    "    num_batches = test_ids.size(1) // seq_length\n",
    "\n",
    "    for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "        inputs = test_ids[:, i: i + seq_length].to(device)\n",
    "        targets = test_ids[:, (i + 1) : (i + 1) + seq_length].to(device)\n",
    "\n",
    "        states = repackage_hidden(states)\n",
    "        outputs, states = model(inputs, states)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # this time, we want to keep track of accumulative loss\n",
    "        total_loss += len(inputs) * loss.item()\n",
    "        count += 1\n",
    "        current_loss = total_loss / count\n",
    "\n",
    "        step = (i + 1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(step, num_batches, current_loss, np.exp(current_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.ckpt')\n",
    "\n",
    "validate(model, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set receives the perplexity around 175.6. I shouldn't have done multi-layer LSTM with dropout training on CPU as it really takes a long time to train."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
